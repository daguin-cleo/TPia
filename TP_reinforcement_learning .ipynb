{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TP reinforcement learning.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4BPOJrE_IzM",
        "colab_type": "text"
      },
      "source": [
        "### **TP 7 Julie Chapdelaine et Cleo Daguin**\n",
        "Le but de ce TP est de comprendre le reinforcement learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0vZeOCgsWLy",
        "colab_type": "code",
        "outputId": "a45bf412-96ab-4044-cbf0-f2a05a782925",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "!pip install gym"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.17.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.18.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.12.0)\n",
            "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a104kwLmtAVk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "from time import sleep\n",
        "import numpy as np\n",
        "import random\n",
        "from IPython.display import clear_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AV3Ft5s3_nz8",
        "colab_type": "text"
      },
      "source": [
        "#### **I) Basic reinforcement learning example, openAI Gym environment**\n",
        "Utilisation du problème de Taxi-v3. Source : https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgWK_FeYu0XT",
        "colab_type": "text"
      },
      "source": [
        "**Sans reinforcement learning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ao_9ttBmtQmU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating the env\n",
        "env = gym.make(\"Taxi-v3\").env\n",
        "\n",
        "env.s = 328"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyJM8kLytR-g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setting the number of iterations, penalties and reward to zero,\n",
        "epochs = 0\n",
        "penalties, reward = 0, 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9a0XL5ptXS9",
        "colab_type": "code",
        "outputId": "a7f49540-b0b0-4e0d-fc62-403acc3cdcee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "frames = []\n",
        "\n",
        "done = False\n",
        "\n",
        "while not done:\n",
        "    action = env.action_space.sample()\n",
        "    state, reward, done, info = env.step(action)\n",
        "\n",
        "    if reward == -10:\n",
        "        penalties += 1\n",
        "\n",
        "    # Put each rendered frame into the dictionary for animation\n",
        "    frames.append({\n",
        "        'frame': env.render(mode='ansi'),\n",
        "        'state': state,\n",
        "        'action': action,\n",
        "        'reward': reward\n",
        "    }\n",
        "    )\n",
        "\n",
        "    epochs += 1\n",
        "\n",
        "print(\"Timesteps taken: {}\".format(epochs))\n",
        "print(\"Penalties incurred: {}\".format(penalties))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Timesteps taken: 378\n",
            "Penalties incurred: 109\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saV67uwLtuzI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Printing all the possible actions, states, rewards.\n",
        "def frames(frames):\n",
        "    for i, frame in enumerate(frames):\n",
        "        clear_output(wait=True)\n",
        "        print(frame['frame'].getvalue())\n",
        "        print(f\"Timestep: {i + 1}\")\n",
        "        print(f\"State: {frame['state']}\")\n",
        "        print(f\"Action: {frame['action']}\")\n",
        "        print(f\"Reward: {frame['reward']}\")\n",
        "        sleep(.1)\n",
        "        \n",
        "frames(frames)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6JSyTyOBkAJ",
        "colab_type": "text"
      },
      "source": [
        "Avec cette méthode, les actions sont choisies au hasard et on peut voir que pour atteindre le but il y a énormément d'actions faites mais surtout beaucoup trop de pénalités."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgvCbkdUusuI",
        "colab_type": "text"
      },
      "source": [
        "**Avec q-learning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbQ-BNYBCO9C",
        "colab_type": "text"
      },
      "source": [
        "Cette méthode va permettre de calculer pour chaque état et pour chaque action possible, son indice de favorabilité à l'atteinte de l'objectif."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21p0Nwaxu886",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Init Taxi-V2 Env\n",
        "env = gym.make(\"Taxi-v3\").env\n",
        "\n",
        "# Init arbitary values\n",
        "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
        "\n",
        "# Hyperparameters\n",
        "alpha = 0.1\n",
        "gamma = 0.6\n",
        "epsilon = 0.1\n",
        "\n",
        "\n",
        "all_epochs = []\n",
        "all_penalties = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asbI9mW0xyBh",
        "colab_type": "code",
        "outputId": "a4a15b19-22c1-4382-e4d3-b998b43ae22c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#Training to fill in the q-table\n",
        "for i in range(1, 100001):\n",
        "    state = env.reset()\n",
        "\n",
        "    # Init Vars\n",
        "    epochs, penalties, reward, = 0, 0, 0\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            # Check the action space\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            # Check the learned values\n",
        "            action = np.argmax(q_table[state])\n",
        "\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "\n",
        "        old_value = q_table[state, action]\n",
        "        next_max = np.max(q_table[next_state])\n",
        "\n",
        "        # Update the new value\n",
        "        new_value = (1 - alpha) * old_value + alpha * \\\n",
        "            (reward + gamma * next_max)\n",
        "        q_table[state, action] = new_value\n",
        "\n",
        "        if reward == -10:\n",
        "            penalties += 1\n",
        "\n",
        "        state = next_state\n",
        "        epochs += 1\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        clear_output(wait=True)\n",
        "        print(\"Episode: {i}\")\n",
        "\n",
        "print(\"Training finished.\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode: {i}\n",
            "Training finished.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpD9xoY-yiDl",
        "colab_type": "code",
        "outputId": "fa209122-9295-474e-eb9c-7e2dcb61267f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "q_table[328]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ -2.40474003,  -2.27325184,  -2.39254947,  -2.34728348,\n",
              "       -11.00109089, -10.5969929 ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmDF-6w_2nzF",
        "colab_type": "text"
      },
      "source": [
        "**Q-learning choix du next action avec la meilleur Q-value**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glIEcPml2anQ",
        "colab_type": "code",
        "outputId": "c8becabb-dcef-4377-b617-bfda267d0d2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "total_epochs, total_penalties = 0, 0\n",
        "episodes = 100\n",
        "\n",
        "for _ in range(episodes):\n",
        "    state = env.reset()\n",
        "    epochs, penalties, reward = 0, 0, 0\n",
        "    \n",
        "    done = False\n",
        "    \n",
        "    while not done:\n",
        "        action = np.argmax(q_table[state])\n",
        "        state, reward, done, info = env.step(action)\n",
        "\n",
        "        if reward == -10:\n",
        "            penalties += 1\n",
        "\n",
        "        epochs += 1\n",
        "\n",
        "    total_penalties += penalties\n",
        "    total_epochs += epochs\n",
        "\n",
        "print(f\"Results after {episodes} episodes:\")\n",
        "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
        "print(f\"Average penalties per episode: {total_penalties / episodes}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results after 100 episodes:\n",
            "Average timesteps per episode: 13.14\n",
            "Average penalties per episode: 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fz8u-RfH2yOV",
        "colab_type": "text"
      },
      "source": [
        "**Q-learning choix random du next action**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tn7nuB3I2xvt",
        "colab_type": "code",
        "outputId": "1828a208-6034-48ab-e9e1-a3a99a529639",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "total_epochs, total_penalties = 0, 0\n",
        "episodes = 100\n",
        "\n",
        "for _ in range(episodes):\n",
        "    state = env.reset()\n",
        "    epochs, penalties, reward = 0, 0, 0\n",
        "    \n",
        "    done = False\n",
        "    \n",
        "    while not done:\n",
        "        action = (int)(random.random()*6)\n",
        "        state, reward, done, info = env.step(action)\n",
        "\n",
        "        if reward == -10:\n",
        "            penalties += 1\n",
        "\n",
        "        epochs += 1\n",
        "\n",
        "    total_penalties += penalties\n",
        "    total_epochs += epochs\n",
        "\n",
        "print(f\"Results after {episodes} episodes:\")\n",
        "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
        "print(f\"Average penalties per episode: {total_penalties / episodes}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results after 100 episodes:\n",
            "Average timesteps per episode: 2445.38\n",
            "Average penalties per episode: 798.64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2cb3ddrDSPw",
        "colab_type": "text"
      },
      "source": [
        "On peut voir que choisir la meilleure action grâce à la table de Q-value est bien plus rentable que de choisir l'action au hasard. Le seul problème de cette technique est qu'on peut atteindre un extremum local et rester bloquer en un point sans atteindre l'objectif. Pour contrer ce problème plusieurs options existes notamment lors du choix de la prochaine action introduire dans un faible pourcentage des cas un choix aléatoire au lieu du choix le plus logique.\n",
        "On peut quand même observer que le choix de la meilleur Q-value est l'apprentissage le plus rapide et le plus efficace dans la plupart des cas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOeqRLqE6SHt",
        "colab_type": "text"
      },
      "source": [
        "**Q learning avec choix de l'action avec la meilleur Q-value dans la plupart des cas**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CZW4ANH6mao",
        "colab_type": "code",
        "outputId": "aa9941a1-e340-460b-f830-0f1506de8ad1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "total_epochs, total_penalties = 0, 0\n",
        "episodes = 100\n",
        "probaQValue=0.95\n",
        "\n",
        "for _ in range(episodes):\n",
        "    state = env.reset()\n",
        "    epochs, penalties, reward = 0, 0, 0\n",
        "    \n",
        "    done = False\n",
        "    \n",
        "    while not done:\n",
        "      if(random.random()<=probaQValue):\n",
        "        action = np.argmax(q_table[state])\n",
        "\n",
        "      else:\n",
        "        action = (int)(random.random()*6)\n",
        "\n",
        "      state, reward, done, info = env.step(action)\n",
        "\n",
        "      if reward == -10:\n",
        "          penalties += 1\n",
        "\n",
        "      epochs += 1\n",
        "\n",
        "    total_penalties += penalties\n",
        "    total_epochs += epochs\n",
        "\n",
        "print(f\"Results after {episodes} episodes:\")\n",
        "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
        "print(f\"Average penalties per episode: {total_penalties / episodes}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results after 100 episodes:\n",
            "Average timesteps per episode: 14.15\n",
            "Average penalties per episode: 0.39\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "077HXTSrLF4a",
        "colab_type": "text"
      },
      "source": [
        "#### **II)Frozen lake problem**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRyYyMHcNmA2",
        "colab_type": "text"
      },
      "source": [
        "Q-learning pour un problème moins complexe, FrozenLake"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5s5M4IZLHo4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Init Frozen lake Env\n",
        "env = gym.make(\"FrozenLake-v0\").env\n",
        "\n",
        "# Init arbitary values\n",
        "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
        "\n",
        "# Hyperparameters\n",
        "alpha = 0.1\n",
        "gamma = 0.6\n",
        "epsilon = 0.1\n",
        "\n",
        "\n",
        "all_epochs = []\n",
        "all_penalties = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNkTWO1oMEJ2",
        "colab_type": "code",
        "outputId": "b4fea736-898e-40e0-d0a1-dfa4e49c738c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#Training to fill in the q-table\n",
        "for i in range(1, 100001):\n",
        "    state = env.reset()\n",
        "\n",
        "    # Init Vars\n",
        "    epochs, penalties, reward, = 0, 0, 0\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            # Check the action space\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            # Check the learned values\n",
        "            action = np.argmax(q_table[state])\n",
        "\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "\n",
        "        old_value = q_table[state, action]\n",
        "        next_max = np.max(q_table[next_state])\n",
        "\n",
        "        # Update the new value\n",
        "        new_value = (1 - alpha) * old_value + alpha * \\\n",
        "            (reward + gamma * next_max)\n",
        "        q_table[state, action] = new_value\n",
        "\n",
        "        if reward == -10:\n",
        "            penalties += 1\n",
        "\n",
        "        state = next_state\n",
        "        epochs += 1\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        clear_output(wait=True)\n",
        "        print(\"Episode: {i}\")\n",
        "\n",
        "print(\"Training finished.\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode: {i}\n",
            "Training finished.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqWgZrYiMXnO",
        "colab_type": "code",
        "outputId": "58786f50-6101-474b-c917-7950b1270683",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "q_table[4]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.00230967, 0.0011711 , 0.00113283, 0.00094806])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5iSwe0jMZFE",
        "colab_type": "code",
        "outputId": "350abf2f-f400-4299-9064-dec2f031cac1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "total_epochs, total_penalties = 0, 0\n",
        "episodes = 100\n",
        "\n",
        "for _ in range(episodes):\n",
        "    state = env.reset()\n",
        "    epochs, penalties, reward = 0, 0, 0\n",
        "    \n",
        "    done = False\n",
        "    \n",
        "    while not done:\n",
        "        action = np.argmax(q_table[state])\n",
        "        state, reward, done, info = env.step(action)\n",
        "\n",
        "        if reward == -10:\n",
        "            penalties += 1\n",
        "\n",
        "        epochs += 1\n",
        "\n",
        "    total_penalties += penalties\n",
        "    total_epochs += epochs\n",
        "\n",
        "print(f\"Results after {episodes} episodes:\")\n",
        "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
        "print(f\"Average penalties per episode: {total_penalties / episodes}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results after 100 episodes:\n",
            "Average timesteps per episode: 10.89\n",
            "Average penalties per episode: 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}