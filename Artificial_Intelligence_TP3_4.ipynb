{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Artificial Intelligence TP3-4.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-ifJGlLuyBl",
        "colab_type": "text"
      },
      "source": [
        "### TP 3-4 Julie Chapdelaine et Cleo Daguin\n",
        "Le but de ce TP est de se familiariser avec les réseaux neuronaux réccurents et de travailler sur de la classification de textes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkR_kKKGlKET",
        "colab_type": "text"
      },
      "source": [
        "#### **I) Reccurent neural network : IMBD sentiment classification**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnfra704xwXe",
        "colab_type": "text"
      },
      "source": [
        "Nous allons d'abord commencer par étudier le réseau LSTM appliqué à un commentaire de film pour qu'il indique si celui ci est positif ou négatif.\n",
        "Source du code : https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kylp-_d7VGXH",
        "colab_type": "code",
        "outputId": "bf1b7ff4-cab8-4189-965d-7a3331a70ec7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.datasets import imdb\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow.compat.v1 as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2.2.0-rc2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWHm-BLezIzq",
        "colab_type": "text"
      },
      "source": [
        "Initialisation des constantes du réseau avec le nombre de mots différents pris en compte dans le réseau, la taille des entrées et le nombre d'éléments pris en compte à chaque itération du model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3FUSKqfVGXN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Nombre de mots pris en compte\n",
        "max_features = 20000\n",
        "## Taille des entrées\n",
        "maxlen = 80\n",
        "## Nombre d'éléments pour chaque epoch du model\n",
        "batch_size = 32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJy-flIUzkqU",
        "colab_type": "text"
      },
      "source": [
        "**Téléchargement des données et séparation** entre les données d'entrainement (train) et de test (test) et entre les entrées (x) et les sorties(y).\n",
        "Les données téléchargées sont sous la forme d'un tableau d'indices des mots du commentaire et d'un chiffre binaire qui indique si le commentaire est positif ou négatif. Les mots sont enregistrés dans un dictionnaire et les indices correspondent au classement des mots en fonction de leur fréquence : le mot avec l'indice 1 est le mot le plus courant. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_h2LHuhVGXR",
        "colab_type": "code",
        "outputId": "b5a8f8cc-e8f1-461f-aee4-5d35430d96b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "print('Loading data...')\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "print(len(x_train), 'train sequences')\n",
        "print(len(x_test), 'test sequences')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "25000 train sequences\n",
            "25000 test sequences\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6_s136y0s1u",
        "colab_type": "text"
      },
      "source": [
        "Utilisation de pad_sequence pour que toutes les entrées aient la même taille maxlen définie plus haut soit en troncant si la taille de l'entrée est plus grande soit en rajoutant des valeurs neutres."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXxXGkDVVGXU",
        "colab_type": "code",
        "outputId": "0ec48b9e-d24c-40fc-b221-475fc3a203ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print('Pad sequences (samples x time)')\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pad sequences (samples x time)\n",
            "x_train shape: (25000, 80)\n",
            "x_test shape: (25000, 80)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_NqsNye1pIR",
        "colab_type": "text"
      },
      "source": [
        "**Création du model.**\n",
        "\n",
        "*   Utilisation de **Embedding** pour faire du plongement lexical soit ne plus définir chaque mot par un nombre (car cela ne permet pas de faire des recoupements entre mots différents mais proches) mais par un vecteur en fonction de son contexte. Ainsi on peut trouver des similitudes entre certains mots. \n",
        "    *   Le premier argument de la fonction indique qu'on ne va prendre en compte qu'un certain nombre de mots qui vont etre les plus courants dans le dictionnaire (ici les 2000 premiers) et remplacer les autres par une valeur neutre (ici 0). Ainsi, il y a moins de mots à traiter donc un traitement plus rapide et moins le mot est courant moins il sera présent et donc moins il sera utile pour savoir si le commentaire est négatif ou positif. \n",
        "    *   Le deuxième paramètre indique la taille du vecteur qui va représenter le mot.\n",
        "*   Utilisation de **LSTM** car il y a beaucoup d'informations différentes passées en entrée et peu sont utiles. LSTM va permettre de faire le tri entre les informations et ne garder que les utiles alors qu'un réseau neuronal plus classique va utiliser toutes les informations et va etre surchargé. Le premier argument est la taille et dépend directement de l'argument fourni dans l'embedding. Le dropout va mettre certains poids du réseau à 0 et ainsi éviter de coller trop à l'exemple (overfitting).\n",
        "*   Utilisation de **sigmoid** car il n'y a que 2 sorties, 0 et 1 pour un commentaire négatif ou positif.\n",
        "*   Ce qui change le plus ici par rapport au model du TP précédent sur la même base de données est qu'ici on prend en compte l'ordre dans lequel les mots sont placés les uns par rapport aux autres ce qui va donner un résultat plus sûr mais aussi plus de calculs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGhDaQwbVGXY",
        "colab_type": "code",
        "outputId": "10e74e6e-2ccd-4c92-f676-cfd38b8ee048",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "print('Build model...')\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, 128))\n",
        "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Build model...\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m00NhfNc_nzo",
        "colab_type": "text"
      },
      "source": [
        "**Compilation du model**\n",
        "*    Utilisation de **binary_crossentropy** car utilisation de sigmoid plus haut."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsVnEBqqVGXb",
        "colab_type": "code",
        "outputId": "c319c8bd-927c-4622-c794-b89b886dec75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7Vz_9VvS-Ni",
        "colab_type": "text"
      },
      "source": [
        "**Application du réseau** sur les données de training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWsgjl8cVGXf",
        "colab_type": "code",
        "outputId": "55193554-af40-423b-b061-9e6b2e82ca15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        }
      },
      "source": [
        "print('Train...')\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=3,\n",
        "          validation_data=(x_test, y_test))\n",
        "score, acc = model.evaluate(x_test, y_test,\n",
        "                            batch_size=batch_size)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train...\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/3\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "25000/25000 [==============================] - 161s 6ms/step - loss: 0.4546 - acc: 0.7859 - val_loss: 0.3779 - val_acc: 0.8356\n",
            "Epoch 2/3\n",
            "25000/25000 [==============================] - 160s 6ms/step - loss: 0.3036 - acc: 0.8776 - val_loss: 0.3869 - val_acc: 0.8336\n",
            "Epoch 3/3\n",
            "25000/25000 [==============================] - 159s 6ms/step - loss: 0.2150 - acc: 0.9163 - val_loss: 0.4147 - val_acc: 0.8340\n",
            "25000/25000 [==============================] - 22s 877us/step\n",
            "Test score: 0.41466219365119933\n",
            "Test accuracy: 0.83404\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEhPHa1GYinH",
        "colab_type": "text"
      },
      "source": [
        "Training loss is smaller than test loss. It means that our network is likely to overfit. To prevent it, we will increase the dropout."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhtJKnVxYXgM",
        "colab_type": "text"
      },
      "source": [
        "**Modification des paramètres**\n",
        "\n",
        "*   Augmentation du dropout. Ainsi, certains poids seront mis à 0 et cela réduira le risque de trop coller aux données d'entrainement et de faire un overfitting\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhTA_AZMVGXi",
        "colab_type": "code",
        "outputId": "c46817c8-7ae3-4876-a892-69e6d50e6fc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "print('Build model...')\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, 128))\n",
        "model.add(LSTM(128, dropout=0.8, recurrent_dropout=0.6))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "print('Train...')\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=3,\n",
        "          validation_data=(x_test, y_test))\n",
        "score, acc = model.evaluate(x_test, y_test,\n",
        "                            batch_size=batch_size)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Build model...\n",
            "Train...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/3\n",
            "25000/25000 [==============================] - 136s 5ms/step - loss: 0.5748 - accuracy: 0.6956 - val_loss: 0.4258 - val_accuracy: 0.8152\n",
            "Epoch 2/3\n",
            "25000/25000 [==============================] - 135s 5ms/step - loss: 0.4444 - accuracy: 0.7997 - val_loss: 0.3938 - val_accuracy: 0.8270\n",
            "Epoch 3/3\n",
            "25000/25000 [==============================] - 136s 5ms/step - loss: 0.3928 - accuracy: 0.8284 - val_loss: 0.3908 - val_accuracy: 0.8335\n",
            "25000/25000 [==============================] - 22s 890us/step\n",
            "Test score: 0.39081519746780397\n",
            "Test accuracy: 0.8334800004959106\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ursc1SFisLfc",
        "colab_type": "text"
      },
      "source": [
        "On peut voir que la val_loss diminue à chaque epoch. L'accuracy n'est qu'a 83 % mais elle augment bien à chaque epoch et pour avoir une meilleure accuracy, il suffit juste de faire plus d'epochs, ce que nous n'avons pas fait car c'est un processus très long."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igRKtPY7TY0u",
        "colab_type": "text"
      },
      "source": [
        "#### **II) Text classification: the Ohsumed dataset**\n",
        "Nous allons suivre les mêmes étapes pour entrainer un réseau de neurones réccurents qui détermine parmi 23 catégories le sujet d'un article médical."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRV688WgVK4n",
        "colab_type": "text"
      },
      "source": [
        "Initialisation des constantes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnUUX2M3VTLH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Nombre de mots pris en compte\n",
        "max_features = 20000\n",
        "## Taille des entrées\n",
        "maxlen = 80\n",
        "## Nombre d'éléments pour chaque epoch du model\n",
        "batch_size = 32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dx84UmSfT-iN",
        "colab_type": "text"
      },
      "source": [
        "Récupération et traitement des données."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YufmZ8KUD44",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_info(path: str):\n",
        "    data = list(os.walk(path))[1:]\n",
        "    files = []\n",
        "    for d in data:\n",
        "        folder_name = d[0]\n",
        "        for file in d[2]:\n",
        "            files.append((folder_name.split('/')[-1], os.path.join(folder_name, file)))\n",
        "\n",
        "    d = defaultdict(int)\n",
        "    texts = defaultdict(list)\n",
        "    for (cate, file) in files:\n",
        "        with open(file, 'r') as outfile:\n",
        "            text = outfile.read()\n",
        "            texts[cate].append(text)\n",
        "            words = text_to_word_sequence(text)\n",
        "            for word in words:\n",
        "                d[word] += 1\n",
        "    words = sorted(d.items(), key=lambda x: x[1], reverse=True)\n",
        "    return (texts, words)\n",
        "\n",
        "def load_data(texts, words_index):\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    index_word_start = 20\n",
        "    index_word_end = 10000\n",
        "    x = []\n",
        "    y = []\n",
        "    step = 0\n",
        "\n",
        "    for categorie, articles in texts.items():\n",
        "        for article in articles:\n",
        "            article_indexed = []\n",
        "            tokens = tokenizer.tokenize(article)\n",
        "            for token in tokens:\n",
        "                index_token = 0\n",
        "                try:\n",
        "                    index_token = 1 + words_index[index_word_start:index_word_end].index(token)\n",
        "                except:\n",
        "                    pass\n",
        "                article_indexed.append(index_token)\n",
        "            x.append(article_indexed)\n",
        "            y.append(list(categories.keys()).index(categorie))\n",
        "    \n",
        "    return (np.array(x), np.array(y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5H_uHYMxU1JM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "texts_train, words_train = get_info('./ohsumed-first-20000-docs/training')\n",
        "texts_test, words_test = get_info('./ohsumed-first-20000-docs/test')\n",
        "\n",
        "words_index = []\n",
        "for word, count in words_train:\n",
        "    words_index.append(word)\n",
        "\n",
        "(x_train, y_train) = load_data(texts_train, words_index)\n",
        "(x_test, y_test) = load_data(texts_test, words_index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32i5fTzvVoOg",
        "colab_type": "text"
      },
      "source": [
        "Utilisation de pad sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lzhh03mRVxda",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Pad sequences (samples x time)')\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXq1AjarV8vi",
        "colab_type": "text"
      },
      "source": [
        "##### Création du model\n",
        "La majeur différence ici est du au fait qu'ilne s'agisse plus d'une sortie binaire mais de multiples sorties binaires. Le nombre de sorties qui doit etre égale au nombre de catégories. On a également ajouter une couche de neurones pour un apprentissage plus fin, ce qui est logique avec une augmentation des sorties. On a aussi rajouter un dropout pour éviter l'overfitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-m5JRDqSV-ue",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(max_features, 128))\n",
        "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(128,activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(23, activation='sofmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ln-r16URWLhe",
        "colab_type": "text"
      },
      "source": [
        "Compilation et entrainement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxh1DmJsWImi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=3,\n",
        "          validation_data=(x_test, y_test))\n",
        "score, acc = model.evaluate(x_test, y_test,\n",
        "                            batch_size=batch_size)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}